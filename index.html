<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-QEEMTJJSV3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-QEEMTJJSV3');
    </script>
	  <meta name=viewport content=“width=800”>
	  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
	  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    li:not(:last-child) {
        margin-bottom: -10px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #ffffd0;
    }

    img {
      border-radius: 15px;
    }
  </style>
  <link rel="icon" type="image/png" href="psihijatar-footer-1.png">
  <title>Daniele Gammelli</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Daniele Gammelli</name>
              </p>
              <p>I am a postdoc at Stanford University, where I work with <a target="_blank" href="https://web.stanford.edu/~pavone/">Marco Pavone</a> on deep learning, reinforcement learning and robotics, with an emphasis on autonomous aerospace vehicles and future mobility systems.</p>

              <p>I completed my PhD at the Technical University of Denmark (DTU), working with <a target="_blank" href="https://camara.scripts.mit.edu/home/">Francisco Camara Pereira</a>, <a target="_blank" href="https://fprodrigues.com/">Filipe Rodrigues</a>, and <a target="_blank" href="https://orbit.dtu.dk/en/persons/dario-pacino">Dario Pacino</a>. During my PhD, I was fortunate to spend my time in the <a target="_blank" href="https://mlsm.man.dtu.dk/">Machine Learning for Smart Mobility</a> Lab. Before my PhD, I spent some time in Amazon's Operations Research Team.
              </p>
              <p align=center>
                <a href="mailto:gammelli@stanford.edu">Email</a> &nbsp/&nbsp
                <a target="_blank" href="https://twitter.com/DanieleGammelli"> Twitter </a> &nbsp/&nbsp
                <a target="_blank" href="https://scholar.google.com/citations?user=C9ZbB3cAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a target="_blank" href="https://github.com/DanieleGammelli">Github</a> &nbsp/&nbsp
                <a target="_blank" href="resume/resume_gammelli.pdf">CV</a> &nbsp/&nbsp
                <a target="_blank" href="www.linkedin.com/in/daniele-gammelli"> LinkedIn </a>
              </p>
            </td>
            <td width="33%">
              <img src="profile_gammelli.jpg" width="250" height="250">
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	      <tr>
	        <td width="100%" valign="middle">
	          <heading>News</heading>
	          <p>
	            <ul>
                <li> [Dec 2023] Our work on <a target="_blank" href="https://rendezvoustransformer.github.io/">Transformers for trajectory optimization</a> was accepted to IEEE Aerospace Conference! </li> <br>
                <li> [Sep 2023] New <a target="_blank" href="https://arxiv.org/abs/2311.05780">preprint</a> on Graph Reinforcement Learning for Electric Autonomous Mobility-on-Demand Systems. </li> <br>
	             <li> [Apr 2023] Our <a target="_blank" href="https://arxiv.org/abs/2305.09129">work</a> on reinforcement learning for network optimization will be presented at ICML 2023! </li> <br>
                <li> [Apr 2023] Our course on <a target="_blank" href="https://stanfordasl.github.io//aa203/sp2223/">Optimal and Learning-based Control</a> at Stanford begins! Check the course webpage for all course material. </li> <br>
            </p>
          </td>
        </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Teaching</heading>
              <p>
                <a target="_blank" href="https://stanfordasl.github.io//aa203/sp2223/">Stanford AA203: Optimal and Learning-based Control</a> - Spring 2023
              </p>
            </td>
          </tr>
          </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
              <p>
                I am interested in the capability of autonomous agents to develop broadly intelligent behavior through learning and interaction, aiming for a reliable and beneficial deployment of AI in the real world. Towards this goal, I focus on machine learning, reinforcement learning, and optimal control for decision-making.
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

        <tr  onmouseout="art_start()" onmouseover="art_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'art_image'><img src='resources/art.gif' width="160" height="100"></div>
            <img src='resources/art.png' width="160" height="100" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function arto_start() {
            document.getElementById('art_image').style.opacity = "1";
            }
            function arto_stop() {
            document.getElementById('art_image').style.opacity = "0";
            }
            // spirl_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://rendezvoustransformer.github.io/">
            <papertitle>Transformers for Trajectory Optimization with Application to Spacecraft Rendezvous</papertitle>
            </a>
            <br>
            <a target="_blank" href="https://slab.stanford.edu/people/tommaso-guffanti">Tommaso Guffanti</a>*, <strong>Daniele Gammelli</strong>*, <a target="_blank" href="https://slab.stanford.edu/people/simone-damico">Simone D'Amico</a>, <a target="_blank" href="https://web.stanford.edu/~pavone/">Marco Pavone</a><br>
            <i>IEEE Aerospace Conference</i>, 2024<br>
            <a target="_blank" href ="https://rendezvoustransformer.github.io/">project page</a> / <a target="_blank" href ="https://arxiv.org/abs/2310.13831">arXiv</a>  / 
            <a target="_blank" href ="https://github.com/Stanford-CAESAR">code</a>
            <br>
          </p>
          <p> We introduce the Autonomous Rendezvous Transformer (ART) for spacecraft trajectory optimization. ART combines optimization-based and AI-based methods, which improves task performance while providing the safety assurances needed for space operations. The method entails embedding high-capacity (namely, transformer-based) neural network models within the optimization process for trajectory generation.</p>
          <p>
          </p>
          </td>
        </tr>

        <tr  onmouseout="grl_start()" onmouseover="grl_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'rtx_image'><img src='resources/grl.png' width="160" height="160"></div>
            <img src='resources/grl.png' width="160" height="160" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function grl_start() {
            document.getElementById('rtx_image').style.opacity = "1";
            }
            function grl_stop() {
            document.getElementById('rtx_image').style.opacity = "0";
            }
            // spirl_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://sites.google.com/stanford.edu/graph-rl-for-network-control/">
            <papertitle>Graph Reinforcement Learning for Network Control via Bi-Level Optimization</papertitle>
            </a>
            <br>
            <strong>Daniele Gammelli</strong>, <a target="_blank" href="https://web.stanford.edu/~jh2/">James Harrison</a>, <a target="_blank" href="https://sites.google.com/view/kaidiyang/">Kaidi Yang</a>, <a target="_blank" href="https://fprodrigues.com/">Filipe Rodrigues</a>, <a target="_blank" href="https://camara.scripts.mit.edu/home/">Francisco C. Pereira</a>, <a target="_blank" href="https://web.stanford.edu/~pavone/">Marco Pavone</a><br>
            <i>International Conference on Machine Learning (ICML)</i>, 2023<br>
            <a target="_blank" href ="https://sites.google.com/stanford.edu/graph-rl-for-network-control/">project page</a> / <a target="_blank" href ="https://arxiv.org/abs/2305.09129">arXiv</a>  / 
            <a target="_blank" href ="https://github.com/DanieleGammelli/graph-rl-for-network-optimization">code</a>
            <br>
          </p>
          <p> We propose a learning-based framework to handle a broad class of network problems by exploiting the main strengths of graph representation learning, reinforcement learning, and classical operations research tools.</p>
          <p>
          </p>
          </td>
        </tr>

        <tr  onmouseout="grl_eamod_start()" onmouseover="grl_eamod_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'rtx_image'><img src='resources/grl_eamod.png' width="160" height="160"></div>
            <img src='resources/grl_eamod.png' width="160" height="160" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function grl_eamod_start() {
            document.getElementById('rtx_image').style.opacity = "1";
            }
            function grl_eamod_stop() {
            document.getElementById('rtx_image').style.opacity = "0";
            }
            // spirl_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://arxiv.org/abs/2311.05780">
            <papertitle>Real-time Control of Electric Autonomous Mobility-on-Demand Systems via Graph Reinforcement Learning</papertitle>
            </a>
            <br>
            <a target="_blank" href="https://www.aaryan-singhal.com/">Aaryan Singhal</a>, <strong>Daniele Gammelli</strong>, <a target="_blank" href="https://profiles.stanford.edu/justin-luke">Justin Luke</a>, <a target="_blank" href="https://karthikg.mit.edu/about-me">Karthik Gopalakrishnan</a>, <a target="_blank" href="https://danielegammelli.github.io/">Dominik Helmreich</a>, <a target="_blank" href="https://web.stanford.edu/~pavone/">Marco Pavone</a><br>
            <i>arXiv</i>, 2023<br>
            <a target="_blank" href ="https://arxiv.org/abs/2311.05780">arXiv</a>  / 
            <a target="_blank" href ="https://github.com/StanfordASL/graph-rl-for-eamod">code</a>
            <br>
          </p>
          <p> We present the E-AMoD control problem through the lens of reinforcement learning and propose a graph network-based framework to achieve drastically improved scalability and performance over heuristics.</p>
          <p>
          </p>
          </td>
        </tr>

        <tr  onmouseout="pato_start()" onmouseover="pato_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'pato_image'><img src='resources/pato.gif' width="160" height="160"></div>
            <img src='resources/pato.png' width="160" height="160" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function pato_start() {
            document.getElementById('pato_image').style.opacity = "1";
            }
            function pato_stop() {
            document.getElementById('pato_image').style.opacity = "0";
            }
            // spirl_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://clvrai.github.io/pato">
            <papertitle>Assisted Teleoperation for Scalable Robot Data Collection</papertitle>
            </a>
            <br>
            <a target="_blank" href="https://shivindass.github.io">Shivin Dass</a>*, <strong>Karl Pertsch</strong>*, <a target="_blank" href="https://www.hejiazhang.me/">Hejia Zhang</a>, <a target="_blank" href="https://youngwoon.github.io">Youngwoon Lee</a>, <a target="_blank" href="https://www.clvrai.com/">Joseph J. Lim</a>, <a target="_blank" href="https://stefanosnikolaidis.net/">Stefanos Nikolaidis</a><br>
            <!-- <i>Conference on Robot Learning (CoRL)</i>, 2022<br> -->
            <a target="_blank" href ="https://clvrai.github.io/pato">project page</a> / <a target="_blank" href ="https://drive.google.com/file/d/1pqgBTju0p7-mUUGl1IYoh4gQ1q_qbL7m/view?usp=sharing">arXiv</a>  / 
            <a target="_blank" href ="https://github.com/clvrai/pato">code</a>
            <br>
          </p>
          <p> We enable scalable robot data collection by assisting human teleoperators with a learned policy. Our approach estimates its uncertainty over future actions to determine when to request user input. In real world user studies we demonstrate that our system enables more efficient teleoperation with reduced mental load and up to four robots in parallel.</p>
          <p>
          </p>
          </td>
        </tr>

        <tr  onmouseout="tarp_start()" onmouseover="tarp_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'tarp_image'><img src='resources/tarp.png' width="160" height="160"></div>
            <img src='resources/tarp.png' width="160" height="160" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function tarp_start() {
            document.getElementById('tarp_image').style.opacity = "1";
            }
            function tarp_stop() {
            document.getElementById('tarp_image').style.opacity = "0";
            }
            // spirl_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://clvrai.com/tarp">
            <papertitle>Task-Induced Representation Learning</papertitle>
            </a>
            <br>
            <a target="_blank" href="https://junjungoal.github.io/">Jun Yamada</a>, <strong>Karl Pertsch</strong>, <a target="_blank" href="https://anisha2102.github.io/">Anisha Gunjal</a>, <a target="_blank" href="https://viterbi-web.usc.edu/~limjj/">Joseph J. Lim</a><br>
            <i>International Conference on Learning Representations (ICLR)</i>, 2022<br>
            <a target="_blank" href ="https://clvrai.com/tarp">project page</a> / <a target="_blank" href ="https://openreview.net/forum?id=OzyXtIZAzFv">arXiv</a>  / 
            <a target="_blank" href ="https://github.com/clvrai/tarp">code</a>
            <br>
          </p>
          <p> We evaluate the effectiveness of representation learning approaches on visually complex environments with substantial distractors. We compare common unsupervised representation learning approaches to task-induced representations, that leverage task information from prior tasks to learn what parts of the scene are important to model and what parts can be ignored.</p>
          <p>
          </p>
          </td>
        </tr>

        <tr  onmouseout="simpl_start()" onmouseover="simpl_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'simpl_image'><img src='resources/simpl_teaser.png' width="160" height="160"></div>
            <img src='resources/simpl_teaser.png' width="160" height="160" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function simpl_start() {
            document.getElementById('simpl_image').style.opacity = "1";
            }
            function simpl_stop() {
            document.getElementById('simpl_image').style.opacity = "0";
            }
            // spirl_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://clvrai.com/simpl">
            <papertitle>Skill-based Meta-Reinforcement Learning</papertitle>
            </a>
            <br>
            <a target="_blank" href="https://namsan96.github.io/">Taewook Nam</a>, <a target="_blank" href="https://shaohua0116.github.io/">Shao-Hua Sun</a>, <strong>Karl Pertsch</strong>, <a target="_blank" href="http://www.sungjuhwang.com/">Sung Ju Hwang</a>, <a target="_blank" href="https://viterbi-web.usc.edu/~limjj/">Joseph J. Lim</a><br>
            <i>International Conference on Learning Representations (ICLR)</i>, 2022<br>
            <a target="_blank" href ="https://clvrai.com/simpl">project page</a> / <a target="_blank" href ="https://openreview.net/forum?id=jeLW-Fh9bV">arXiv</a>  / 
            <a target="_blank" href ="https://github.com/namsan96/simpl">code</a>
            <br>
          </p>
          <p> We perform meta-RL on top of skills extracted from large task-agnostic offline datasets. By combining meta-training tasks with offline data we can meta-learn policies that can quickly learn new long-horizon, sparse reward tasks.</p>
          <p>
          </p>
          </td>
        </tr>

        <tr  onmouseout="skild_start()" onmouseover="skild_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'skild_image'><img src='skild_thumbnail.gif' width="160" height="160"></div>
            <img src='skild_start.jpeg' width="160" height="160" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function skild_start() {
            document.getElementById('skild_image').style.opacity = "1";
            }
            function skild_stop() {
            document.getElementById('skild_image').style.opacity = "0";
            }
            // spirl_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://arxiv.org/abs/2107.10253">
            <papertitle>Demonstration-Guided Reinforcement Learning with Learned Skills</papertitle>
            </a>
            <br>
            <strong>Karl Pertsch</strong>, <a target="_blank" href="https://youngwoon.github.io/">Youngwoon Lee</a>, <a target="_blank" href="https://ventusyue.github.io/">Yue Wu</a>, <a target="_blank" href="https://viterbi-web.usc.edu/~limjj/">Joseph J. Lim</a><br>
            <i>Conference on Robot Learning (CoRL)</i>, 2021<br>
            <a target="_blank" href ="https://clvrai.github.io/skild/">project page</a> / <a target="_blank" href ="https://arxiv.org/abs/2107.10253">arXiv</a>  / 
            <a target="_blank" href ="https://github.com/clvrai/skild">code</a>
            <br>
          </p>
          <p> We follow long-horizon demonstrations by imitating the demonstrated skills instead of the primitive actions. By using skills learned from large, task-agnostic experience datasets for imitation, our approach SkiLD can seamlessly integrate task-agnostic data & demonstrations via a skill-based learning framework.</p>
          <p>
          </p>
          </td>
        </tr>

        <tr  onmouseout="spirl_start()" onmouseover="spirl_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'spirl_image'><img src='spirl_corl2020.gif' width="160" height="160"></div>
            <img src='spirl_start.jpeg' width="160" height="160" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function spirl_start() {
            document.getElementById('spirl_image').style.opacity = "1";
            }
            function spirl_stop() {
            document.getElementById('spirl_image').style.opacity = "0";
            }
            // spirl_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://arxiv.org/abs/2010.11944">
            <papertitle>Accelerating Reinforcement Learning with Learned Skill Priors</papertitle>
            </a>
            <br>
            <strong>Karl Pertsch</strong>, <a target="_blank" href="https://youngwoon.github.io/">Youngwoon Lee</a>, <a target="_blank" href="https://viterbi-web.usc.edu/~limjj/">Joseph J. Lim</a><br>
            <i>Conference on Robot Learning (CoRL)</i>, 2020 (<span style="color: #ff0000">Plenary Talk, top 4%</span>)<br>
            <i>Workshop on Robot Learning @ NeurIPS</i>, 2020 (<span style="color: #ff0000">Best Paper Runner-up Award</span>)<br>
            <i>Deep RL Workshop @ NeurIPS</i>, 2020 (<span style="color: #ff0000">Oral</span>)<br>
            <a target="_blank" href ="https://clvrai.github.io/spirl/">project page</a> / <a target="_blank" href ="https://arxiv.org/abs/2010.11944">arXiv</a>  / 
            <a target="_blank" href ="https://github.com/clvrai/spirl">code</a>
            <br>
          </p>
          <p> We jointly learn an embedding space of skills and a prior over skills. This skill prior tells us <b>when</b> to use <b>which</b> skill and guides learning on new tasks for effective skill transfer from large offline datasets.</p>
          <p>
          </p>
          </td>
        </tr>

        <tr  onmouseout="mopa_start()" onmouseover="mopa_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'mopa_image'><img src='mopa_corl2020.gif' width="160" height="160"></div>
            <img src='mopa_start.png' width="160" height="160" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function mopa_start() {
            document.getElementById('mopa_image').style.opacity = "1";
            }
            function mopa_stop() {
            document.getElementById('mopa_image').style.opacity = "0";
            }
            // mopa_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://arxiv.org/abs/2010.11940">
            <papertitle>Motion Planner Augmented Reinforcement Learning for Robot Manipulation in Obstructed Environments</papertitle>
            </a>
            <br>
            <a target="_blank" href="https://www.junjungoal.tech/">Jun Yamada</a>*, <a target="_blank" href="https://youngwoon.github.io/">Youngwoon Lee</a>*, <a target="_blank" href="https://www.gautamsalhotra.com/">Gautam Salhorta</a>, <strong>Karl Pertsch</strong>, <a target="_blank" href="https://mpflueger.github.io/">Max Pflueger</a>, <a target="_blank" href="http://robotics.usc.edu/~gaurav/">Gaurav S.Sukhatme</a>, <a target="_blank" href="https://viterbi-web.usc.edu/~limjj/">Joseph J. Lim</a>, <a target="_blank" href="http://www.peter-englert.net/">Peter Englert</a><br>
            <i>Conference on Robot Learning (CoRL)</i>, 2020<br>
            <a target="_blank" href ="https://clvrai.github.io/mopa-rl/">project page</a> / <a target="_blank" href ="https://arxiv.org/abs/2010.11940">arXiv</a>  / 
            <a target="_blank" href ="https://github.com/clvrai/mopa-rl">code</a>
            <br>
          </p>
          <p> Our approach augments model-free RL agents with motion planning capabilities, enabling them to solve long-horizon manipulation tasks in cluttered environments.</p>
          <p>
          </p>
          </td>
        </tr>

        <tr  onmouseout="gcp_start()" onmouseover="gcp_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'gcp_image'><img src='gcp_animate.gif' width="160" height="160"></div>
            <img src='gcp.jpeg' width="160" height="160" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function gcp_start() {
            document.getElementById('gcp_image').style.opacity = "1";
            }
            function gcp_stop() {
            document.getElementById('gcp_image').style.opacity = "0";
            }
            // gcp_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://arxiv.org/abs/2006.13205">
            <papertitle>Long-Horizon Visual Planning with Goal-Conditioned Hierarchical Predictors</papertitle>
            </a>
            <br>
            <strong>Karl Pertsch</strong>*, <a target="_blank" href="http://www.cis.upenn.edu/~oleh/">Oleh Rybkin</a>*, <a target="_blank" href="https://febert.github.io/">Frederik Ebert</a>, <a target="_blank" href="http://people.eecs.berkeley.edu/~cbfinn/">Chelsea Finn</a>, <a target="_blank" href="https://www.seas.upenn.edu/~dineshj/">Dinesh Jayaraman</a>, <a target="_blank" href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a><br>
            <i>Conference on Neural Information Processing Systems (NeurIPS)</i>, 2020<br>
            <a target="_blank" href ="https://orybkin.github.io/video-gcp/">project page</a> / <a target="_blank" href ="https://arxiv.org/abs/2006.13205">arXiv</a>  /
            <a target="_blank" href ="https://youtu.be/axXx-x86IeY">video</a> / <a target="_blank" href ="https://github.com/orybkin/video-gcp">code</a>
            <br>
          </p>
          <p> We propose a hierarchical prediction model that predicts sequences by recursive infilling. We use this model to devise a hierarchical planning approach that allows to scale visual MPC to long-horizon tasks with hundreds of time steps.</p>
          <p>
          </p>
          </td>
        </tr>

        <tr  onmouseout="keyin_stop()" onmouseover="keyin_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id = 'keyin_image'><img src='keyin.gif' width="160" height="160"></div>
            <img src='keyin.png' width="160" height="160" style="z-index:-1">
            </div>
            <script type="text/javascript">
            function keyin_start() {
            document.getElementById('keyin_image').style.opacity = "1";
            }
            function keyin_stop() {
            document.getElementById('keyin_image').style.opacity = "0";
            }
            keyin_stop()
            </script>
          </td>
          <td width="75%" valign="top">
          <p>
          <p>
            <a target="_blank" href="https://arxiv.org/abs/1904.05869">
            <papertitle>Keyframing the Future: Keyframe Discovery for Visual Prediction and Planning</papertitle>
            </a>
            <br>
            <strong>Karl Pertsch</strong>*, <a target="_blank" href="http://www.cis.upenn.edu/~oleh/">Oleh Rybkin</a>*, <a target="_blank" href="https://www.linkedin.com/in/yjy0625/">Jingyun Yang</a>, Shenghao Zhou, <a target="_blank" href="http://www.scs.ryerson.ca/kosta/">Kosta Derpanis</a>, <a target="_blank" href="http://www-bcf.usc.edu/~limjj/">Joseph Lim</a>, <a target="_blank" href="http://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>, <a target="_blank" href="http://www.drewjaegle.com/">Andrew Jaegle</a><br>
            <i>Conference on Learning for Dynamics and Control</i>, 2020<br>
            <a target="_blank" href ="https://sites.google.com/view/keyin">project page</a> / <a target="_blank" href ="https://arxiv.org/abs/1904.05869">arXiv</a>  /
            <a target="_blank" href ="https://youtu.be/e2hVV5FDKf8">video</a> / <a target="_blank" href ="poster_keyin.pdf">poster</a>
            <br>
          </p>
          <p> We propose a keyframe-based video prediction model that can unsupervisedly discover the moments of interesting change, the <i>keyframes</i>, in the data. We show that using the predicted keyframes as subgoals for planning improves performance on a simulated pushing task.</p>
          <p>
          <i> Hover over image (or tap the screen) to see the video.</i></p>
          </p>
          </td>
        </tr>

	      <tr  onmouseout="sensorimotor_stop()" onmouseover="sensorimotor_start()">
	        <td width="25%">
	          <div class="one">
	          <div class="two" id = 'sensorimotor_image'><img src='CLASP_after.gif' width="160" height="160"></div>
	          <img src='CLASP_before.png' width="160" height="160">
	          </div>
	          <script type="text/javascript">
	          function sensorimotor_start() {
	          document.getElementById('sensorimotor_image').style.opacity = "1";
	          }
	          function sensorimotor_stop() {
	          document.getElementById('sensorimotor_image').style.opacity = "0";
	          }
	          sensorimotor_stop()
	          </script>
	        </td>
	        <td width="75%" valign="top">
	        <p>
	        <p>
	          <a target="_blank" href="https://arxiv.org/abs/1806.09655">
	          <papertitle>Learning what you can do before doing anything</papertitle>
	          </a>
	          <br>
	          <a target="_blank" href="http://www.cis.upenn.edu/~oleh/">Oleh Rybkin</a>*, <strong>Karl Pertsch</strong>*,  <a target="_blank" href="http://www.scs.ryerson.ca/kosta/">Kosta Derpanis</a>, <a target="_blank" href="http://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>, <a target="_blank" href="http://www.drewjaegle.com/">Andrew Jaegle</a><br>
	          <i>International Conference on Learning Representations (ICLR)</i>, 2019<br>
	          <a target="_blank" href ="https://daniilidis-group.github.io/learned_action_spaces/">project page</a> / <a target="_blank" href ="https://arxiv.org/abs/1806.09655">arXiv</a>  /
	          <a target="_blank" href ="https://daniilidis-group.github.io/learned_action_spaces/poster.pdf">poster</a>
	          <br>
	        </p>
	        <p> We learn an agent's action space from pure visual observations along with a predictive model. It can then be used to perform model predictive control, requiring orders of magnitude fewer action annotated videos.</p>
	        <p>
	        <i> Hover over image (or tap the screen) to see the video.</i></p>
	        </p>
	        </td>
	      </tr>

	      <tr>
            <td width="25%">
              <img src='object_pose.png' width="160" height="160">
            </td>
            <td valign="top" width="75%">
              <p>
                <a target="_blank" href="https://arxiv.org/abs/1712.01924">
                  <papertitle>iPose: Instance-Aware 6D Pose Estimation of Partly Occluded Objects</papertitle>
                </a>
                <br>
                <a target="_blank" href="https://scholar.google.com/citations?user=-oPXmWIAAAAJ&hl=en">Omid Hosseini Jafari</a>*, <a target="_blank" href="http://sivakm.github.io/">Siva Karthik Mustikovela</a>*, <strong>Karl Pertsch</strong>,  <a target="_blank" href="https://hci.iwr.uni-heidelberg.de/vislearn/people/eric-brachmann/">Eric Brachmann</a>, <a target="_blank" href="https://hci.iwr.uni-heidelberg.de/vislearn/people/carsten-rother/">Carsten Rother</a><br>
                <i>Asian Conference on Computer Vision (ACCV)</i>, 2018
                <br>
                <p></p>
                <p>Combining a CNN-based regression of dense on-object surface labeling with RANSAC-based pose fitting for accurate 6DoF pose estimation of texture-less objects under heavy occlusion.</p>
            </td>
          </tr>
        </table>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	      <tr>
	        <td>
	        <br>
	        <p align="right">
	          <font size="2">
	          <strong>I borrowed this website layout from <a target="_blank" href="https://jonbarron.info/">here</a>!</strong>
		    </font>
	        </p>
	        </td>
	      </tr>
      	</table>

        <script type="text/javascript">
          var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        </td>
    </tr>
  </table>
</body>

</html>
